{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1) load texts...\n",
      "(2) doc to var...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\yechao\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 65606 unique tokens.\n",
      "Shape of data tensor: (21924, 100)\n",
      "Shape of label tensor: (21924, 12)\n",
      "(3) split data set...\n",
      "train docs: 14031\n",
      "val docs: 3508\n",
      "test docs: 4385\n",
      "(5) training model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 200)          13121400  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12)                2412      \n",
      "=================================================================\n",
      "Total params: 13,444,612\n",
      "Trainable params: 13,444,612\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "['loss', 'acc']\n",
      "Train on 14031 samples, validate on 3508 samples\n",
      "Epoch 1/6\n",
      "14031/14031 [==============================] - 90s 6ms/step - loss: 1.6674 - acc: 0.4133 - val_loss: 1.0348 - val_acc: 0.6331\n",
      "Epoch 2/6\n",
      "14031/14031 [==============================] - 109s 8ms/step - loss: 0.8645 - acc: 0.7105 - val_loss: 0.7173 - val_acc: 0.7705\n",
      "Epoch 3/6\n",
      "14031/14031 [==============================] - 116s 8ms/step - loss: 0.4927 - acc: 0.8543 - val_loss: 0.5115 - val_acc: 0.8481\n",
      "Epoch 4/6\n",
      "14031/14031 [==============================] - 117s 8ms/step - loss: 0.2882 - acc: 0.9212 - val_loss: 0.5073 - val_acc: 0.8535\n",
      "Epoch 5/6\n",
      "14031/14031 [==============================] - 119s 8ms/step - loss: 0.1745 - acc: 0.9513 - val_loss: 0.4428 - val_acc: 0.8874\n",
      "Epoch 6/6\n",
      "14031/14031 [==============================] - 120s 9ms/step - loss: 0.1050 - acc: 0.9722 - val_loss: 0.4868 - val_acc: 0.8814\n",
      "(6) testing model...\n",
      "['loss', 'acc']\n",
      "4385/4385 [==============================] - 10s 2ms/step\n",
      "[0.8122784890769689, 0.8198403648802737]\n",
      "(7) prediction ...\n",
      "(1) load texts...\n",
      "(2) doc to var...\n",
      "Found 65606 unique tokens.\n",
      "Shape of data tensor: (21924, 100)\n",
      "Shape of label tensor: (21924, 12)\n",
      "(3) split data set...\n",
      "train docs: 14031\n",
      "val docs: 3508\n",
      "test docs: 4385\n",
      "(5) training model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 100, 200)          13121400  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100, 200)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 98, 250)           150250    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 32, 250)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8000)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 200)               1600200   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 12)                2412      \n",
      "=================================================================\n",
      "Total params: 14,874,262\n",
      "Trainable params: 14,874,262\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "['loss', 'acc']\n",
      "Train on 14031 samples, validate on 3508 samples\n",
      "Epoch 1/4\n",
      "14031/14031 [==============================] - 63s 4ms/step - loss: 1.3675 - acc: 0.5352 - val_loss: 0.5891 - val_acc: 0.8110\n",
      "Epoch 2/4\n",
      "14031/14031 [==============================] - 62s 4ms/step - loss: 0.3108 - acc: 0.9115 - val_loss: 0.4376 - val_acc: 0.8786\n",
      "Epoch 3/4\n",
      "14031/14031 [==============================] - 63s 4ms/step - loss: 0.0708 - acc: 0.9841 - val_loss: 0.4704 - val_acc: 0.8771\n",
      "Epoch 4/4\n",
      "14031/14031 [==============================] - 63s 4ms/step - loss: 0.0265 - acc: 0.9934 - val_loss: 0.5178 - val_acc: 0.8754\n",
      "(6) testing model...\n",
      "['loss', 'acc']\n",
      "4385/4385 [==============================] - 4s 868us/step\n",
      "[0.7947380075704821, 0.831927023945268]\n",
      "(1) load texts...\n",
      "(2) doc to var...\n",
      "Found 65606 unique tokens.\n",
      "Shape of data tensor: (21924, 100)\n",
      "Shape of label tensor: (21924, 12)\n",
      "(3) split data set...\n",
      "train docs: 14031\n",
      "val docs: 3508\n",
      "test docs: 4385\n",
      "(4) load word2vec as embedding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\yechao\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9213 words not in w2v model\n",
      "(5) training model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 100, 128)          8397696   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 200)               263200    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 12)                2412      \n",
      "=================================================================\n",
      "Total params: 8,663,308\n",
      "Trainable params: 265,612\n",
      "Non-trainable params: 8,397,696\n",
      "_________________________________________________________________\n",
      "['loss', 'acc']\n",
      "Train on 14031 samples, validate on 3508 samples\n",
      "Epoch 1/6\n",
      "14031/14031 [==============================] - 70s 5ms/step - loss: 0.8923 - acc: 0.7254 - val_loss: 0.5182 - val_acc: 0.8452\n",
      "Epoch 2/6\n",
      "14031/14031 [==============================] - 72s 5ms/step - loss: 0.5336 - acc: 0.8352 - val_loss: 0.4085 - val_acc: 0.8737\n",
      "Epoch 3/6\n",
      "14031/14031 [==============================] - 72s 5ms/step - loss: 0.4394 - acc: 0.8642 - val_loss: 0.3737 - val_acc: 0.8851\n",
      "Epoch 4/6\n",
      "14031/14031 [==============================] - 72s 5ms/step - loss: 0.3806 - acc: 0.8831 - val_loss: 0.3587 - val_acc: 0.8937\n",
      "Epoch 5/6\n",
      "14031/14031 [==============================] - 72s 5ms/step - loss: 0.3413 - acc: 0.8944 - val_loss: 0.3434 - val_acc: 0.8928\n",
      "Epoch 6/6\n",
      "14031/14031 [==============================] - 72s 5ms/step - loss: 0.3050 - acc: 0.9059 - val_loss: 0.3271 - val_acc: 0.9031\n",
      "(6) testing model...\n",
      "['loss', 'acc']\n",
      "4385/4385 [==============================] - 9s 2ms/step\n",
      "[0.531274515900128, 0.8513112884834664]\n",
      "(1) load texts...\n",
      "(2) doc to var...\n",
      "Found 65606 unique tokens.\n",
      "Shape of data tensor: (21924, 100)\n",
      "Shape of label tensor: (21924, 12)\n",
      "(3) split data set...\n",
      "train docs: 14031\n",
      "val docs: 3508\n",
      "test docs: 4385\n",
      "(4) load word2vec as embedding...\n",
      "9213 words not in w2v model\n",
      "(5) training model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 100, 128)          8397696   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 100, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 98, 250)           96250     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 32, 250)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 8000)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               1024128   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 12)                1548      \n",
      "=================================================================\n",
      "Total params: 9,519,622\n",
      "Trainable params: 1,121,926\n",
      "Non-trainable params: 8,397,696\n",
      "_________________________________________________________________\n",
      "['loss', 'acc']\n",
      "Train on 14031 samples, validate on 3508 samples\n",
      "Epoch 1/4\n",
      "14031/14031 [==============================] - 21s 1ms/step - loss: 1.5631 - acc: 0.6667 - val_loss: 0.5640 - val_acc: 0.8327\n",
      "Epoch 2/4\n",
      "14031/14031 [==============================] - 20s 1ms/step - loss: 0.4642 - acc: 0.8633 - val_loss: 0.4598 - val_acc: 0.8666\n",
      "Epoch 3/4\n",
      "14031/14031 [==============================] - 20s 1ms/step - loss: 0.3028 - acc: 0.9083 - val_loss: 0.4537 - val_acc: 0.8700\n",
      "Epoch 4/4\n",
      "14031/14031 [==============================] - 20s 1ms/step - loss: 0.1997 - acc: 0.9377 - val_loss: 0.4644 - val_acc: 0.8734\n",
      "(6) testing model...\n",
      "['loss', 'acc']\n",
      "4385/4385 [==============================] - 3s 580us/step\n",
      "[0.7076256387182082, 0.8164196123147093]\n",
      "(7) prediction file...\n",
      "(1) load texts...\n",
      "(2) doc to var...\n",
      "Found 65606 unique tokens.\n",
      "Shape of data tensor: (21924, 65607)\n",
      "Shape of label tensor: (21924, 12)\n",
      "(3) split data set...\n",
      "train docs: 14031\n",
      "val docs: 3508\n",
      "test docs: 4385\n",
      "(5) training model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 256)               16795648  \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 12)                3084      \n",
      "=================================================================\n",
      "Total params: 16,798,732\n",
      "Trainable params: 16,798,732\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "['loss', 'acc']\n",
      "Train on 14031 samples, validate on 3508 samples\n",
      "Epoch 1/4\n",
      "14031/14031 [==============================] - 53s 4ms/step - loss: 0.5591 - acc: 0.8562 - val_loss: 0.3098 - val_acc: 0.9108\n",
      "Epoch 2/4\n",
      "14031/14031 [==============================] - 50s 4ms/step - loss: 0.0614 - acc: 0.9880 - val_loss: 0.3325 - val_acc: 0.9088\n",
      "Epoch 3/4\n",
      "14031/14031 [==============================] - 49s 3ms/step - loss: 0.0146 - acc: 0.9976 - val_loss: 0.3982 - val_acc: 0.9065\n",
      "Epoch 4/4\n",
      "14031/14031 [==============================] - 48s 3ms/step - loss: 0.0057 - acc: 0.9987 - val_loss: 0.4657 - val_acc: 0.9017\n",
      "(6) testing model...\n",
      "['loss', 'acc']\n",
      "4385/4385 [==============================] - 8s 2ms/step\n",
      "[0.8813740284440184, 0.8549600912200684]\n",
      "(1) load texts...\n",
      "(2) doc to var...\n",
      "the shape of train is (17600, 62418)\n",
      "the shape of test is (4324, 62418)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\yechao\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3) Naive Bayes...\n",
      "test acc:0.8501387604070305\n",
      "train acc:0.9561363636363637\n",
      "(1) load texts...\n",
      "(2) doc to var...\n",
      "the shape of train is (17600, 62418)\n",
      "the shape of test is (4324, 62418)\n",
      "(3) SVM...\n",
      "--------------------linear\n",
      "test acc:0.8443570767807586\n",
      "train acc:0.9908522727272727\n"
     ]
    }
   ],
   "source": [
    "def lstm():\n",
    "    VECTOR_DIR = 'wiki.zh.vector.bin'\n",
    "\n",
    "    MAX_SEQUENCE_LENGTH = 100\n",
    "    EMBEDDING_DIM = 200\n",
    "    VALIDATION_SPLIT = 0.16\n",
    "    TEST_SPLIT = 0.2\n",
    "\n",
    "    print('(1) load texts...')\n",
    "    train_texts = open('train_contents.txt', encoding=\"utf8\").read().split('\\n')\n",
    "    train_labels = open('train_labels.txt').read().split('\\n')\n",
    "    test_texts = open('test_contents.txt', encoding=\"utf8\").read().split('\\n')\n",
    "    test_labels = open('test_labels.txt').read().split('\\n')\n",
    "    all_texts = train_texts + test_texts\n",
    "    all_labels = train_labels + test_labels\n",
    "\n",
    "    print('(2) doc to var...')\n",
    "    from keras.preprocessing.text import Tokenizer\n",
    "    from keras.preprocessing.sequence import pad_sequences\n",
    "    from keras.utils import to_categorical\n",
    "    import numpy as np\n",
    "\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(all_texts)\n",
    "    sequences = tokenizer.texts_to_sequences(all_texts)\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    labels = to_categorical(np.asarray(all_labels))\n",
    "    print('Shape of data tensor:', data.shape)\n",
    "    print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "    print('(3) split data set...')\n",
    "    p1 = int(len(data) * (1 - VALIDATION_SPLIT - TEST_SPLIT))\n",
    "    p2 = int(len(data) * (1 - TEST_SPLIT))\n",
    "    x_train = data[:p1]\n",
    "    y_train = labels[:p1]\n",
    "    x_val = data[p1:p2]\n",
    "    y_val = labels[p1:p2]\n",
    "    x_test = data[p2:]\n",
    "    y_test = labels[p2:]\n",
    "    print('train docs: ' + str(len(x_train)))\n",
    "    print('val docs: ' + str(len(x_val)))\n",
    "    print('test docs: ' + str(len(x_test)))\n",
    "\n",
    "    print('(5) training model...')\n",
    "    from keras.layers import Dense, Input, Flatten, Dropout, GlobalAveragePooling1D\n",
    "    from keras.layers import LSTM, Embedding\n",
    "    from keras.models import Sequential\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(word_index) + 1, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n",
    "    # model.add(GlobalAveragePooling1D())\n",
    "    model.add(LSTM(200, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(labels.shape[1], activation='softmax'))\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['acc'])\n",
    "    print(model.metrics_names)\n",
    "    model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=6, batch_size=128)\n",
    "    # model.save('lstm.h5')\n",
    "\n",
    "    print('(6) testing model...')\n",
    "    print(model.metrics_names)\n",
    "    print(model.evaluate(x_test, y_test))\n",
    "\n",
    "    print('(7) prediction ...')\n",
    "    import pickle\n",
    "    lstm_preds = model.predict(x_test)\n",
    "\n",
    "    return  lstm_preds\n",
    "\n",
    "def cnn():\n",
    "    # coding:utf-8\n",
    "\n",
    "    VECTOR_DIR = 'wiki.zh.vector.bin'\n",
    "\n",
    "    MAX_SEQUENCE_LENGTH = 100\n",
    "    EMBEDDING_DIM = 200\n",
    "    VALIDATION_SPLIT = 0.16\n",
    "    TEST_SPLIT = 0.2\n",
    "\n",
    "    print('(1) load texts...')\n",
    "    train_texts = open('train_contents.txt', encoding=\"utf8\").read().split('\\n')\n",
    "    train_labels = open('train_labels.txt').read().split('\\n')\n",
    "    test_texts = open('test_contents.txt', encoding=\"utf8\").read().split('\\n')\n",
    "    test_labels = open('test_labels.txt').read().split('\\n')\n",
    "    all_texts = train_texts + test_texts\n",
    "    all_labels = train_labels + test_labels\n",
    "\n",
    "    print('(2) doc to var...')\n",
    "    from keras.preprocessing.text import Tokenizer\n",
    "    from keras.preprocessing.sequence import pad_sequences\n",
    "    from keras.utils import to_categorical\n",
    "    import numpy as np\n",
    "\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(all_texts)\n",
    "    sequences = tokenizer.texts_to_sequences(all_texts)\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    labels = to_categorical(np.asarray(all_labels))\n",
    "    print('Shape of data tensor:', data.shape)\n",
    "    print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "    print('(3) split data set...')\n",
    "    p1 = int(len(data) * (1 - VALIDATION_SPLIT - TEST_SPLIT))\n",
    "    p2 = int(len(data) * (1 - TEST_SPLIT))\n",
    "    x_train = data[:p1]\n",
    "    y_train = labels[:p1]\n",
    "    x_val = data[p1:p2]\n",
    "    y_val = labels[p1:p2]\n",
    "    x_test = data[p2:]\n",
    "    y_test = labels[p2:]\n",
    "    print('train docs: ' + str(len(x_train)))\n",
    "    print('val docs: ' + str(len(x_val)))\n",
    "    print('test docs: ' + str(len(x_test)))\n",
    "\n",
    "    print('(5) training model...')\n",
    "    from keras.layers import Dense, Input, Flatten, Dropout\n",
    "    from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "    from keras.models import Sequential\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(word_index) + 1, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv1D(250, 3, padding='valid', activation='relu', strides=1))\n",
    "    model.add(MaxPooling1D(3))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(EMBEDDING_DIM, activation='relu'))\n",
    "    model.add(Dense(labels.shape[1], activation='softmax'))\n",
    "    model.summary()\n",
    "    # plot_model(model, to_file='model.png',show_shapes=True)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  # optimizer='rmsprop',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "    print(model.metrics_names)\n",
    "    model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=4, batch_size=128)\n",
    "    # model.save('cnn.h5')\n",
    "\n",
    "    print('(6) testing model...')\n",
    "    print(model.metrics_names)\n",
    "    print(model.evaluate(x_test, y_test))\n",
    "\n",
    "    cnn_preds = model.predict(x_test)\n",
    "    return cnn_preds\n",
    "\n",
    "def pre_lstm():\n",
    "    # coding:utf-8\n",
    "\n",
    "    VECTOR_DIR = 'wiki.zh.vector.bin'\n",
    "\n",
    "    MAX_SEQUENCE_LENGTH = 100\n",
    "    EMBEDDING_DIM = 128\n",
    "    VALIDATION_SPLIT = 0.16\n",
    "    TEST_SPLIT = 0.2\n",
    "\n",
    "    print('(1) load texts...')\n",
    "    train_texts = open('train_contents.txt', encoding=\"utf8\").read().split('\\n')\n",
    "    train_labels = open('train_labels.txt').read().split('\\n')\n",
    "    test_texts = open('test_contents.txt', encoding=\"utf8\").read().split('\\n')\n",
    "    test_labels = open('test_labels.txt').read().split('\\n')\n",
    "    all_texts = train_texts + test_texts\n",
    "    all_labels = train_labels + test_labels\n",
    "\n",
    "    print('(2) doc to var...')\n",
    "    from keras.preprocessing.text import Tokenizer\n",
    "    from keras.preprocessing.sequence import pad_sequences\n",
    "    from keras.utils import to_categorical\n",
    "    import numpy as np\n",
    "\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(all_texts)\n",
    "    sequences = tokenizer.texts_to_sequences(all_texts)\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    labels = to_categorical(np.asarray(all_labels))\n",
    "    print('Shape of data tensor:', data.shape)\n",
    "    print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "    print('(3) split data set...')\n",
    "    p1 = int(len(data) * (1 - VALIDATION_SPLIT - TEST_SPLIT))\n",
    "    p2 = int(len(data) * (1 - TEST_SPLIT))\n",
    "    x_train = data[:p1]\n",
    "    y_train = labels[:p1]\n",
    "    x_val = data[p1:p2]\n",
    "    y_val = labels[p1:p2]\n",
    "    x_test = data[p2:]\n",
    "    y_test = labels[p2:]\n",
    "    print('train docs: ' + str(len(x_train)))\n",
    "    print('val docs: ' + str(len(x_val)))\n",
    "    print('test docs: ' + str(len(x_test)))\n",
    "\n",
    "    print('(4) load word2vec as embedding...')\n",
    "    import gensim\n",
    "    from keras.utils import plot_model\n",
    "    w2v_model = gensim.models.KeyedVectors.load_word2vec_format(VECTOR_DIR, binary=True)\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "    not_in_model = 0\n",
    "    in_model = 0\n",
    "    for word, i in word_index.items():\n",
    "        if str(word) in w2v_model:\n",
    "            in_model += 1\n",
    "            embedding_matrix[i] = np.asarray(w2v_model[str(word)], dtype='float32')\n",
    "        else:\n",
    "            not_in_model += 1\n",
    "    print(str(not_in_model) + ' words not in w2v model')\n",
    "    from keras.layers import Embedding\n",
    "    embedding_layer = Embedding(len(word_index) + 1,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=False)\n",
    "\n",
    "    print('(5) training model...')\n",
    "    from keras.layers import Dense, Input, Flatten, Dropout\n",
    "    from keras.layers import LSTM, Embedding\n",
    "    from keras.models import Sequential\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    model.add(LSTM(200, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(labels.shape[1], activation='softmax'))\n",
    "    model.summary()\n",
    "    # plot_model(model, to_file='model.png',show_shapes=True)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['acc'])\n",
    "    print(model.metrics_names)\n",
    "    model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=6, batch_size=128)\n",
    "    # model.save('word_vector_lstm.h5')\n",
    "\n",
    "    print('(6) testing model...')\n",
    "    print(model.metrics_names)\n",
    "    print(model.evaluate(x_test, y_test))\n",
    "\n",
    "    pre_lstm_preds = model.predict(x_test)\n",
    "\n",
    "    return pre_lstm_preds\n",
    "\n",
    "def pre_cnn():\n",
    "    VECTOR_DIR = 'wiki.zh.vector.bin'\n",
    "\n",
    "    MAX_SEQUENCE_LENGTH = 100\n",
    "    EMBEDDING_DIM = 128\n",
    "    VALIDATION_SPLIT = 0.16\n",
    "    TEST_SPLIT = 0.2\n",
    "\n",
    "    print('(1) load texts...')\n",
    "    train_texts = open('train_contents.txt', encoding=\"utf8\").read().split('\\n')\n",
    "    train_labels = open('train_labels.txt').read().split('\\n')\n",
    "    test_texts = open('test_contents.txt', encoding=\"utf8\").read().split('\\n')\n",
    "    test_labels = open('test_labels.txt').read().split('\\n')\n",
    "    all_texts = train_texts + test_texts\n",
    "    all_labels = train_labels + test_labels\n",
    "\n",
    "    print('(2) doc to var...')\n",
    "    from keras.preprocessing.text import Tokenizer\n",
    "    from keras.preprocessing.sequence import pad_sequences\n",
    "    from keras.utils import to_categorical\n",
    "    import numpy as np\n",
    "\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(all_texts)\n",
    "    sequences = tokenizer.texts_to_sequences(all_texts)\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    labels = to_categorical(np.asarray(all_labels))\n",
    "    print('Shape of data tensor:', data.shape)\n",
    "    print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "    print('(3) split data set...')\n",
    "    # split the data into training set, validation set, and test set\n",
    "    p1 = int(len(data) * (1 - VALIDATION_SPLIT - TEST_SPLIT))\n",
    "    p2 = int(len(data) * (1 - TEST_SPLIT))\n",
    "    x_train = data[:p1]\n",
    "    y_train = labels[:p1]\n",
    "    x_val = data[p1:p2]\n",
    "    y_val = labels[p1:p2]\n",
    "    x_test = data[p2:]\n",
    "    y_test = labels[p2:]\n",
    "    print('train docs: ' + str(len(x_train)))\n",
    "    print('val docs: ' + str(len(x_val)))\n",
    "    print('test docs: ' + str(len(x_test)))\n",
    "\n",
    "    print('(4) load word2vec as embedding...')\n",
    "    import gensim\n",
    "    from keras.utils import plot_model\n",
    "    w2v_model = gensim.models.KeyedVectors.load_word2vec_format(VECTOR_DIR, binary=True)\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "    not_in_model = 0\n",
    "    in_model = 0\n",
    "    for word, i in word_index.items():\n",
    "        if str(word) in w2v_model:\n",
    "            in_model += 1\n",
    "            embedding_matrix[i] = np.asarray(w2v_model[str(word)], dtype='float32')\n",
    "        else:\n",
    "            not_in_model += 1\n",
    "    print(str(not_in_model) + ' words not in w2v model')\n",
    "    from keras.layers import Embedding\n",
    "    embedding_layer = Embedding(len(word_index) + 1,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=False)\n",
    "\n",
    "    print('(5) training model...')\n",
    "    from keras.layers import Dense, Input, Flatten, Dropout\n",
    "    from keras.layers import Conv1D, MaxPooling1D, Embedding, GlobalMaxPooling1D\n",
    "    from keras.models import Sequential\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv1D(250, 3, padding='valid', activation='relu', strides=1))\n",
    "    model.add(MaxPooling1D(3))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(EMBEDDING_DIM, activation='relu'))\n",
    "    model.add(Dense(labels.shape[1], activation='softmax'))\n",
    "    model.summary()\n",
    "    # plot_model(model, to_file='model.png',show_shapes=True)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['acc'])\n",
    "    print(model.metrics_names)\n",
    "    model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=4, batch_size=128)\n",
    "    # model.save('word_vector_cnn.h5')\n",
    "\n",
    "    print('(6) testing model...')\n",
    "    print(model.metrics_names)\n",
    "    print(model.evaluate(x_test, y_test))\n",
    "\n",
    "    print('(7) prediction file...')\n",
    "    import pickle\n",
    "    pre_cnn_preds = model.predict(x_test)\n",
    "\n",
    "    return pre_cnn_preds\n",
    "\n",
    "def mlp():\n",
    "    # coding:utf-8\n",
    "\n",
    "    VECTOR_DIR = 'wiki.zh.vector.bin'\n",
    "\n",
    "    MAX_SEQUENCE_LENGTH = 100\n",
    "    EMBEDDING_DIM = 200\n",
    "    VALIDATION_SPLIT = 0.16\n",
    "    TEST_SPLIT = 0.2\n",
    "\n",
    "    print('(1) load texts...')\n",
    "    train_texts = open('train_contents.txt', encoding=\"utf8\").read().split('\\n')\n",
    "    train_labels = open('train_labels.txt').read().split('\\n')\n",
    "    test_texts = open('test_contents.txt', encoding=\"utf8\").read().split('\\n')\n",
    "    test_labels = open('test_labels.txt').read().split('\\n')\n",
    "    all_texts = train_texts + test_texts\n",
    "    all_labels = train_labels + test_labels\n",
    "\n",
    "    print('(2) doc to var...')\n",
    "    from keras.preprocessing.text import Tokenizer\n",
    "    from keras.preprocessing.sequence import pad_sequences\n",
    "    from keras.utils import to_categorical\n",
    "    import numpy as np\n",
    "\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(all_texts)\n",
    "    sequences = tokenizer.texts_to_sequences(all_texts)\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "    data = tokenizer.sequences_to_matrix(sequences, mode='tfidf')\n",
    "    labels = to_categorical(np.asarray(all_labels))\n",
    "    print('Shape of data tensor:', data.shape)\n",
    "    print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "    print('(3) split data set...')\n",
    "    p1 = int(len(data) * (1 - VALIDATION_SPLIT - TEST_SPLIT))\n",
    "    p2 = int(len(data) * (1 - TEST_SPLIT))\n",
    "    x_train = data[:p1]\n",
    "    y_train = labels[:p1]\n",
    "    x_val = data[p1:p2]\n",
    "    y_val = labels[p1:p2]\n",
    "    x_test = data[p2:]\n",
    "    y_test = labels[p2:]\n",
    "    print('train docs: ' + str(len(x_train)))\n",
    "    print('val docs: ' + str(len(x_val)))\n",
    "    print('test docs: ' + str(len(x_test)))\n",
    "\n",
    "    print('(5) training model...')\n",
    "    from keras.layers import Dense, Input, Flatten, Dropout\n",
    "    from keras.layers import LSTM, Embedding\n",
    "    from keras.models import Sequential\n",
    "    from keras.utils import plot_model\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_shape=(len(word_index) + 1,), activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(labels.shape[1], activation='softmax'))\n",
    "    model.summary()\n",
    "    # plot_model(model, to_file='model.png',show_shapes=True)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['acc'])\n",
    "    print(model.metrics_names)\n",
    "    model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=4, batch_size=128)\n",
    "    # model.save('mlp.h5')\n",
    "\n",
    "    print('(6) testing model...')\n",
    "    print(model.metrics_names)\n",
    "    print(model.evaluate(x_test, y_test))\n",
    "    mlp_preds = model.predict(x_test)\n",
    "\n",
    "    return mlp_preds\n",
    "def bys():\n",
    "    # coding:utf-8\n",
    "\n",
    "    VECTOR_DIR = 'wiki.zh.vector.bin'\n",
    "\n",
    "    MAX_SEQUENCE_LENGTH = 100\n",
    "    EMBEDDING_DIM = 200\n",
    "    TEST_SPLIT = 0.2\n",
    "\n",
    "    print('(1) load texts...')\n",
    "    train_texts = open('train_contents.txt', encoding=\"utf8\").read().split('\\n')\n",
    "    train_labels = open('train_labels.txt').read().split('\\n')\n",
    "    test_texts = open('test_contents.txt', encoding=\"utf8\").read().split('\\n')\n",
    "    test_labels = open('test_labels.txt').read().split('\\n')\n",
    "    all_text = train_texts + test_texts\n",
    "\n",
    "    print('(2) doc to var...')\n",
    "    from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "    count_v0 = CountVectorizer()\n",
    "    counts_all = count_v0.fit_transform(all_text)\n",
    "    count_v1 = CountVectorizer(vocabulary=count_v0.vocabulary_)\n",
    "    counts_train = count_v1.fit_transform(train_texts)\n",
    "    print(\"the shape of train is \" + repr(counts_train.shape))\n",
    "    count_v2 = CountVectorizer(vocabulary=count_v0.vocabulary_)\n",
    "    counts_test = count_v2.fit_transform(test_texts)\n",
    "    print(\"the shape of test is \" + repr(counts_test.shape))\n",
    "\n",
    "    tfidftransformer = TfidfTransformer()\n",
    "    train_data = tfidftransformer.fit(counts_train).transform(counts_train)\n",
    "    test_data = tfidftransformer.fit(counts_test).transform(counts_test)\n",
    "\n",
    "    x_train = train_data\n",
    "    y_train = train_labels\n",
    "    x_test = test_data\n",
    "    y_test = test_labels\n",
    "\n",
    "    print('(3) Naive Bayes...')\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    from sklearn import metrics\n",
    "    clf = MultinomialNB(alpha=1)\n",
    "    clf.fit(x_train, y_train)\n",
    "\n",
    "    # preds = clf.predict(x_test)\n",
    "    # preds = preds.tolist()\n",
    "\n",
    "    test_acc = clf.score(x_test, y_test)\n",
    "    train_acc = clf.score(x_train, y_train)\n",
    "    print('test acc:{}'.format(test_acc))\n",
    "    print('train acc:{}'.format(train_acc))\n",
    "\n",
    "    bys_preds = clf.predict(x_test)\n",
    "    return bys_preds\n",
    "def svm():\n",
    "    # coding:utf-8\n",
    "\n",
    "    VECTOR_DIR = 'wiki.zh.vector.bin'\n",
    "\n",
    "    MAX_SEQUENCE_LENGTH = 100\n",
    "    EMBEDDING_DIM = 200\n",
    "    TEST_SPLIT = 0.2\n",
    "\n",
    "    print('(1) load texts...')\n",
    "    train_texts = open('train_contents.txt', encoding=\"utf8\").read().split('\\n')\n",
    "    train_labels = open('train_labels.txt').read().split('\\n')\n",
    "    test_texts = open('test_contents.txt', encoding=\"utf8\").read().split('\\n')\n",
    "    test_labels = open('test_labels.txt').read().split('\\n')\n",
    "    all_text = train_texts + test_texts\n",
    "\n",
    "    print('(2) doc to var...')\n",
    "    from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "    count_v0 = CountVectorizer();\n",
    "    counts_all = count_v0.fit_transform(all_text);\n",
    "    count_v1 = CountVectorizer(vocabulary=count_v0.vocabulary_);\n",
    "    counts_train = count_v1.fit_transform(train_texts);\n",
    "    print(\"the shape of train is \" + repr(counts_train.shape))\n",
    "    count_v2 = CountVectorizer(vocabulary=count_v0.vocabulary_);\n",
    "    counts_test = count_v2.fit_transform(test_texts);\n",
    "    print(\"the shape of test is \" + repr(counts_test.shape))\n",
    "\n",
    "    tfidftransformer = TfidfTransformer();\n",
    "    train_data = tfidftransformer.fit(counts_train).transform(counts_train);\n",
    "    test_data = tfidftransformer.fit(counts_test).transform(counts_test);\n",
    "\n",
    "    x_train = train_data\n",
    "    y_train = train_labels\n",
    "    x_test = test_data\n",
    "    y_test = test_labels\n",
    "\n",
    "    print('(3) SVM...')\n",
    "    from sklearn.svm import SVC\n",
    "    # svclf = SVC(c=1.0,kernel = 'linear')\n",
    "    # kernels = ['linear','poly','rbf','sigmoid']\n",
    "    kernels = ['linear']\n",
    "\n",
    "    for kernel in kernels:\n",
    "        print('-' * 20 + kernel)\n",
    "        svclf = SVC(C=1.0, kernel=kernel)\n",
    "        svclf.fit(x_train, y_train)\n",
    "\n",
    "        test_acc = svclf.score(x_test, y_test)\n",
    "        train_acc = svclf.score(x_train, y_train)\n",
    "        print('test acc:{}'.format(test_acc))\n",
    "        print('train acc:{}'.format(train_acc))\n",
    "\n",
    "        svm_preds = svclf.predict(x_test)\n",
    "    return svm_preds\n",
    "\n",
    "\n",
    "lstm_preds = lstm()\n",
    "cnn_preds = cnn()\n",
    "pre_lstm_preds = pre_lstm()\n",
    "pre_cnn_preds = pre_cnn()\n",
    "mlp_preds = mlp()\n",
    "bys_preds = bys()\n",
    "svm_preds = svm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "all_pred1 = pd.DataFrame([list(lstm_preds1),list(cnn_preds1),list(pre_lstm_preds1),list(pre_cnn_preds1),list(mlp_preds1),list(bys_preds),list(svm_preds)])\n",
    "# pred = all_pred.mode(axis=1).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm_preds = lstm()\n",
    "cnn_preds = cnn()\n",
    "pre_lstm_preds = pre_lstm()\n",
    "pre_cnn_preds = pre_cnn()\n",
    "mlp_preds = mlp()\n",
    "bys_preds = bys()\n",
    "svm_preds = svm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm_preds1 = list(map(lambda x:x.argmax(),lstm_preds))  \n",
    "cnn_preds1 = list(map(lambda x:x.argmax(),cnn_preds))  \n",
    "pre_lstm_preds1 = list(map(lambda x:x.argmax(),pre_lstm_preds))  \n",
    "pre_cnn_preds1 = list(map(lambda x:x.argmax(),pre_cnn_preds))  \n",
    "mlp_preds1 =list(map(lambda x:x.argmax(),mlp_preds))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4385, 12)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00114512, 0.03445483, 0.00200022, 0.00404092, 0.00741396,\n",
       "       0.01502563, 0.01248523, 0.00431627, 0.00325661, 0.00684551,\n",
       "       0.06282775, 0.84618795], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4324"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bys_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pred.drop([5,6],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.ndarray' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-e09022abd7c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mall_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.ndarray' object is not callable"
     ]
    }
   ],
   "source": [
    "prediction = all_pred.mode(axis=0).iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>4375</th>\n",
       "      <th>4376</th>\n",
       "      <th>4377</th>\n",
       "      <th>4378</th>\n",
       "      <th>4379</th>\n",
       "      <th>4380</th>\n",
       "      <th>4381</th>\n",
       "      <th>4382</th>\n",
       "      <th>4383</th>\n",
       "      <th>4384</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 4385 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  0    1    2    3    4    5    6    7    8    9    ...  4375 4376 4377 4378  \\\n",
       "0   11    4    8   10    9    9   10    6    4    2 ...   4.0  2.0  8.0  9.0   \n",
       "1    1    4    8   10    9    9    5    6    4    2 ...   4.0  2.0  8.0  9.0   \n",
       "2    9    4    8   10    9    9   11    6    4    2 ...   4.0  2.0  8.0  9.0   \n",
       "3    9    4    8   10    9    9   11    6    4    2 ...   4.0  2.0  8.0  9.0   \n",
       "4    9    4    8   10    9    9    8    6    4    2 ...   4.0  2.0  8.0  9.0   \n",
       "\n",
       "  4379  4380  4381  4382 4383 4384  \n",
       "0  5.0  10.0  11.0  10.0  1.0  3.0  \n",
       "1  5.0  11.0  11.0  10.0  1.0  3.0  \n",
       "2  5.0   9.0   6.0  10.0  1.0  5.0  \n",
       "3  5.0  10.0   6.0  10.0  1.0  5.0  \n",
       "4  5.0  10.0   6.0  10.0  1.0  2.0  \n",
       "\n",
       "[5 rows x 4385 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_labels = open('test_labels.txt').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4324"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>4375</th>\n",
       "      <th>4376</th>\n",
       "      <th>4377</th>\n",
       "      <th>4378</th>\n",
       "      <th>4379</th>\n",
       "      <th>4380</th>\n",
       "      <th>4381</th>\n",
       "      <th>4382</th>\n",
       "      <th>4383</th>\n",
       "      <th>4384</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows Ã— 4385 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1     2     3     4     5     6     7     8     9     ...   4375  \\\n",
       "0    11     4     8    10     9     9    10     6     4     2  ...      4   \n",
       "1     1     4     8    10     9     9     5     6     4     2  ...      4   \n",
       "2     9     4     8    10     9     9    11     6     4     2  ...      4   \n",
       "3     9     4     8    10     9     9    11     6     4     2  ...      4   \n",
       "4     9     4     8    10     9     9     8     6     4     2  ...      4   \n",
       "5     0     0     0     0     0     0     0     0     0     0  ...      4   \n",
       "6     0     0     0     0     0     0     0     0     0     0  ...      4   \n",
       "\n",
       "   4376  4377  4378  4379  4380  4381  4382  4383  4384  \n",
       "0     2     8     9     5    10    11    10     1     3  \n",
       "1     2     8     9     5    11    11    10     1     3  \n",
       "2     2     8     9     5     9     6    10     1     5  \n",
       "3     2     8     9     5    10     6    10     1     5  \n",
       "4     2     8     9     5    10     6    10     1     2  \n",
       "5     2     8     9     5    10     6    10     1     5  \n",
       "6     2     8     9     5     9    11    10     1     9  \n",
       "\n",
       "[7 rows x 4385 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "bys_preds1=([0]*61) + list(bys_preds)\n",
    "svm_preds1=([0]*61) + list(svm_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pred1 = pd.DataFrame([list(lstm_preds1),list(cnn_preds1),list(pre_lstm_preds1),list(pre_cnn_preds1),list(mlp_preds1),list(bys_preds1),list(svm_preds1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svm_preds1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4385"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "([0]*61) + list(bys_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\yechao\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\pandas\\core\\algorithms.py:762: UserWarning: Unable to sort modes: unorderable types: numpy.ndarray() > numpy.str_()\n",
      "  warn(\"Unable to sort modes: %s\" % e)\n",
      "c:\\users\\yechao\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\pandas\\core\\algorithms.py:762: UserWarning: Unable to sort modes: unorderable types: numpy.ndarray() < numpy.str_()\n",
      "  warn(\"Unable to sort modes: %s\" % e)\n"
     ]
    }
   ],
   "source": [
    "prediction = all_pred1.iloc[:,61:].mode().iloc[0,:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4324"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.0, 6.0, 2.0, ..., 10.0, 1.0, 3.0], dtype=object)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(list(map(lambda x: int(x),test_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  6,  1, ..., 10,  1,  2])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3747"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((labels - prediction)==0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = prediction.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.866558741905643"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3747/4324"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
