{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "import jieba.analyse\n",
    "\n",
    "# stc='你知不知道我知道你知道这件事'\n",
    "# stc='你知不知道不知道我知道你知道这件事是不好的'\n",
    "stc='你知不知道不知道我知道你知道这件事是不好的，OK？'\n",
    "seq=jieba.cut(stc)\n",
    "# '/'.join(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\jieba.cache\n",
      "Loading model cost 0.841 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你\n",
      "知不知道\n",
      "不\n",
      "知道\n",
      "我\n",
      "知道\n",
      "你\n",
      "知道\n",
      "这件\n",
      "事是\n",
      "不好\n",
      "的\n",
      "，\n",
      "OK\n",
      "？\n"
     ]
    }
   ],
   "source": [
    "for word in seq:\n",
    "    print('%s' % word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stc='先看算法导论，证明直接跳过，看文字和伪代码，如果不懂在看另一本。万一看懂了呢？我当年就是在皇家理工的分手亭上，以成为程序员而不是搞ACM或者研究算法的目标，把算法导论的英文第二版看完的，并不觉得有什么死的很惨的感觉。'\n",
    "keywords1=jieba.analyse.extract_tags(stc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['算法',\n",
       " '导论',\n",
       " 'ACM',\n",
       " '程序员',\n",
       " '很惨',\n",
       " '跳过',\n",
       " '先看',\n",
       " '理工',\n",
       " '分手',\n",
       " '英文',\n",
       " '一本',\n",
       " '代码',\n",
       " '皇家',\n",
       " '万一',\n",
       " '文字',\n",
       " '证明',\n",
       " '当年',\n",
       " '感觉',\n",
       " '第二',\n",
       " '目标']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "基于 理解 的 分词 方法 这种 分词 方法 是 通过 让 计算机 模拟 人 对 句子 的 理解 达到 识别 词 的 效果 其 基本 思想 就是 在 分词 的 同时 进行 句法 语义 分析 利用 句法 信息 和 语义 信息 来 处理 歧义 现象 它 通常 包括 三个 部分 分词 子系统 句法 语义 子系统 总控 部分 在 总控 部分 的 协调 下 分词 子系统 可以 获得 有关 词句 子 等 的 句法 和 语义 信息 来 对 分词 歧义 进行 判断 即 它 模拟 了 人 对 句子 的 理解 过程 这种 分词 方法 需要 使用 大量 的 语言 知识 和 信息 由于 汉语 语言 知识 的 笼统 复杂性 难以 将 各种 语言 信息 组织 成 机器 可 直接 读取 的 形式 因此 目前 基于 理解 的 分词 系统 还 处在 试验 阶段\n",
      "--------------------\n",
      "基于 统计 的 分词 方法 给出 大量 已经 分词 的 文本 利用 统计 机器 学习 模型 学习 词语切分 的 规律 称为 训练 从而 实现 对 未知 文本 的 切分 例如 最大 概率 分词 方法 和 最大 熵 分词 方法 等 随着 大规模 语料库 的 建立 统计 机器 学习 方法 的 研究 和 发展 基于 统计 的 中文 分词 方法 渐渐 成为 了 主流 方法\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "r = '[’!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+'\n",
    "r = '[，。：（）、]'\n",
    "with open('raw.txt','r')as f:\n",
    "    for line in f:\n",
    "        line = re.sub(r, '',line)\n",
    "        seg = jieba.cut(line.strip())\n",
    "        seg = ' '.join(seg)\n",
    "        print(seg)\n",
    "        print('--'*10)\n",
    "#         for word in seg:\n",
    "#             print(word)\n",
    "        with open('cuted.txt','w')as fw:\n",
    "#             for word in list(seg):\n",
    "#                 fw.write(word.encode('utf-8'))\n",
    "            fw.write(seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}